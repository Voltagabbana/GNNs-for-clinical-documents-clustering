{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dda74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916cd1f",
   "metadata": {},
   "source": [
    "// codice che usavo prima (importavo tutto il codice pesante, ora importo solo il corpus finale!!)\n",
    "\n",
    "time1 = time.time()\n",
    "import TextImport\n",
    "time2 = time.time()\n",
    "print((time2-time1))\n",
    "\n",
    "Primo Import iniziato alle 13:21 , finito alle 13:33 (800 secondi)\n",
    "\n",
    "Secondo import 580 secondi\n",
    "\n",
    "Terzo Import 581 secondi\n",
    "\n",
    "La cosa strana è che runnando TextImport su visualStudio ci mette 250 secondi! (ovvero runnare è più\n",
    "veloce che importare!?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be336e",
   "metadata": {},
   "source": [
    "# Rappresentazione del corpus usando i word embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1fa623",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_filtrato15_11.pkl', 'rb') as file:\n",
    "    # Carica i dati dal file\n",
    "    corpus_filtrato = pickle.load(file) # corpus filtrato è il corpus senza le parole inutili"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7808b6d",
   "metadata": {},
   "source": [
    "Corpus_filtrato è stato prodotto dal file \"TextImportOriginale\" nella cartella \"word embeddings\", vado lì se voglio modificare qualcosa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf33ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_vecs_preProcessingSenzaParoleInutili.p\", \"rb\") as file:\n",
    "    word2Vec_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29233674",
   "metadata": {},
   "source": [
    "Vedo Word2VecDatiIMPORT nella cartella WordEmbedding per avere un'idea di come usare gli embedding e di come sono stati fatti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8d849",
   "metadata": {},
   "source": [
    "In seguito spiego la corrispondenza univoca fra i vettori numerici (embeddings) delle parole del modello e le parole presenti in Vocabolario preso da \"textImport\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea5cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_names = word2Vec_model.wv.index_to_key # word names è la stringa di parola corrispondente all'embedding numerico\n",
    "# (Traduzione Embedding -> stringa)\n",
    "word_vectors = [word2Vec_model.wv[word] for word in word_names]\n",
    "# word_vectors è l'embedding relativo alle parole di word_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3f8c2",
   "metadata": {},
   "source": [
    "Osservazione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce8a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParoleMancanti = [parola for parola in word_names if parola not in TextImport.Vocabolario]\n",
    "#print(len(word_names))\n",
    "#print(len(TextImport.ListaParoleInutili))\n",
    "#print(len(word_names)-len(TextImport.ListaParoleInutili))\n",
    "#len(TextImport.Vocabolario)\n",
    "\n",
    "# Risultato importantissimo, prima mi veniva che word_names (ovvero il vettore delle parole sputate dal modello di gensim) e \n",
    "# il mio Vocabolario avessero lunghezza lievemente diversa. Questo era perchè in precedenza, sbagliando avevo allenato il modello \n",
    "# senza togliere le parole inutili. Ora invece tutto torna!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daed6cfd",
   "metadata": {},
   "source": [
    "Ora traduco il corpus (lista di liste di parole ovvero lista di documenti) traducendo ogni parola in embedding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1acc73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filtrato_embedding = []\n",
    "for doc in corpus_filtrato:\n",
    "    doc_embedding = []\n",
    "    for parola in doc:\n",
    "        doc_embedding.append(word2Vec_model.wv[parola])\n",
    "    corpus_filtrato_embedding.append(doc_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79eeb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_filtrato_embedding[0][0]\n",
    "# ho fatto diverse prove e sembra proprio che tutto il procedimento di traduzione sia andato a buon fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb10b3",
   "metadata": {},
   "source": [
    "Corpus_filtrato_embedding è il corpus formato solo dalle parole utili, dove ogni parola è un vettore di dim 10 (embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c66c904",
   "metadata": {},
   "source": [
    "# Costruzione grafo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1d665",
   "metadata": {},
   "source": [
    "Iniziamo con il modello più semplice possibile, ovvero i collegamenti di una parola sono solo con la precedente e con la successiva\n",
    "\n",
    "\n",
    "Per capire come costruire il grafo prendo modelli->INFOGRAPH->data-> MUTAG e ne copio la struttura.\n",
    "\n",
    "Consideriamo il primo documento, da trasformare in un grafo, da questo tutti gli altri documenti verranno modificati con lo stesso procedimento, l'unica differenza è la lunghezza del documento ma non è un problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48bb9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc0 = corpus_filtrato_embedding[0]\n",
    "Doc0 = np.array(Doc0) # meglio averceli in numpy per fare le operazionui!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cba99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apre un file in modalità scrittura\n",
    "with open('Graph0_A.txt', 'w') as file:\n",
    "    # Lista di coppie di valori\n",
    "    values = []\n",
    "    for i in range(len(Doc0)):\n",
    "        if i == 0:\n",
    "            val = (i,i+1)\n",
    "            values.append(val)\n",
    "        elif i == len(Doc0)-1:\n",
    "            val = (i,i-1)\n",
    "            values.append(val)\n",
    "        else:\n",
    "            val1= (i,i-1)\n",
    "            values.append(val1)\n",
    "            val2 = (i,i+1)\n",
    "            values.append(val2)\n",
    "\n",
    "    # Itera sulla lista e scrive le coppie nel file\n",
    "    for value in values:\n",
    "        file.write(f\"{value[0]}, {value[1]}\\n\")\n",
    "        \n",
    "## abbiamo quindi creato la lista di adiacenza \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5249c41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684ecd2de66b4d4fac79d7203dcdb918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('E3C_node_attributes.txt', 'w') as file:\n",
    "    values = []\n",
    "    doc = corpus_filtrato_embedding[0]\n",
    "    for i in range(len(doc)):\n",
    "        values.append(doc[i])\n",
    "        \n",
    "\n",
    "    for value in tqdm(values):\n",
    "        for j in range(len(value)):\n",
    "            if(j == len(value)-1):\n",
    "                file.write(f\"{value[j]}\\n\")\n",
    "            else:\n",
    "                file.write(f\"{value[j]}, \")\n",
    "\n",
    "## abbiamo quindi creato la feature matrix \"X\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170a577",
   "metadata": {},
   "source": [
    "Il codice appena scritto crea il .txt equivalente _A e il file equivalente a _node_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dde73",
   "metadata": {},
   "source": [
    "Generalizziamo quanto fatto su un documento per inserire tutti i documenti!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86b9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apre un file in modalità scrittura\n",
    "with open('E3C_A.txt', 'w') as file:\n",
    "    # Lista di coppie di valori\n",
    "    values = []\n",
    "    prev_count = 0\n",
    "    for doc in corpus_filtrato_embedding:\n",
    "        \n",
    "        for i in range(len(doc)):\n",
    "        \n",
    "            if i == 0:\n",
    "                val = (i+prev_count,i+1+prev_count)\n",
    "                values.append(val)\n",
    "            elif i == len(doc)-1:\n",
    "                val = (i+prev_count,i-1+prev_count)\n",
    "                values.append(val)\n",
    "            else:\n",
    "                val1= (i+prev_count,i-1+prev_count)\n",
    "                values.append(val1)\n",
    "                val2 = (i+prev_count,i+1+prev_count)\n",
    "                values.append(val2)\n",
    "        prev_count += len(doc)-1\n",
    "\n",
    "    # Itera sulla lista e scrive le coppie nel file\n",
    "    for value in values:\n",
    "        file.write(f\"{value[0]}, {value[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83163a",
   "metadata": {},
   "source": [
    "Così facendo abbiamo creato il file.A (lista di adiacenza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb46d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apre un file in modalità scrittura\n",
    "with open('E3C_graph_indicator.txt', 'w') as file:\n",
    "    values = []\n",
    "    for ind,doc in enumerate(corpus_filtrato_embedding):\n",
    "        \n",
    "        for i in range(len(doc)):\n",
    "            values.append(ind)\n",
    "        \n",
    "    for value in values:\n",
    "        file.write(f\"{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63934946",
   "metadata": {},
   "source": [
    "così facendo abbiamo creato il file .graph_indicator che dice quali nodi appartengono a quali grafi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceeac617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11068563, -0.5326252 ,  0.86516297, ..., -0.06655114,\n",
       "        -0.17851014,  0.21890694],\n",
       "       [-0.51522696, -1.0387008 ,  1.1703496 , ..., -0.3740427 ,\n",
       "        -0.5833328 , -0.22025317],\n",
       "       [-0.6063753 ,  0.3164312 ,  1.7273585 , ...,  0.6947744 ,\n",
       "        -0.9461107 , -0.25950187],\n",
       "       ...,\n",
       "       [-0.45306423, -0.6907194 ,  0.6036256 , ..., -0.3809749 ,\n",
       "         0.02652384, -0.36844888],\n",
       "       [-0.7124144 , -0.49716508,  1.0456138 , ..., -0.13115153,\n",
       "        -0.5543687 ,  0.16673848],\n",
       "       [-0.3750048 ,  0.30667436,  1.9497802 , ...,  0.2292165 ,\n",
       "         0.03109064,  0.2720961 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Doc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acf0b7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc77db4e59c469fb0502886685fa57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b53cbe404a49b99c39f094d8f8b41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7699720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('E3C_node_attributes.txt', 'w') as file:\n",
    "    values = []\n",
    "    for doc in tqdm(corpus_filtrato_embedding):\n",
    "        for i in range(len(doc)):\n",
    "            values.append(doc[i])\n",
    "        \n",
    "\n",
    "    for value in tqdm(values):\n",
    "        for j in range(len(value)):\n",
    "            if(j == len(value)-1):\n",
    "                file.write(f\"{value[j]}\\n\")\n",
    "            else:\n",
    "                file.write(f\"{value[j]}, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878b57e",
   "metadata": {},
   "source": [
    "Così facendo abbiamo creato il file .node_attributes che ci dice le features di tutti i nodi (ovvero i coefficienti degli embedding delle parole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb204c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
